---
title: "OLS Mathematical"
author: "H Emre Kale"
date: "1/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Now we will explore the mathematical derivation of ordinary least squares parameter estimation of the  linear models. If we want to express using matrix notation:

$$y = X\beta + \epsilon$$
$$\epsilon \sim N(0,\sigma)$$
where $y$ is the observation vector, $X$ is the design matrix, $\beta$'s are the coefficients and \epsilon is the error. Then we want to find $\beta$ such that it minimises the total error.

$$\arg\min_{\beta} \epsilon^T\epsilon \text{, where}  $$
$$ \epsilon = y - X\beta $$
and

$$\arg \min_{\beta}(y - X\beta)^T(y - X\beta)$$
or in vector notation.

$$(y - X\beta)^T(y - X\beta) = y^Ty - y^TX\beta - (X\beta)^Ty + (X\beta)^T X\beta$$
Remember $(AB)^T = B^TA^T$ so:

$$(y^T(X\beta))^T =  (X\beta)^T y$$
so the second and third term of the equation are transpose of each other and they are scalars. Since transpose of a scalar is the same scalar then, both of the terms are same so:

$$(y - X\beta)^T(y - X\beta) = y^Ty - 2(X\beta)^Ty + \beta^T X^T X\beta$$
This is a quadratic equation so to minimize wrt $\beta$ we take the partial derivative of the equation.
As a first step  let's define the vector N X 1 vector b $$\frac{\partial b^TAb}{\partial b}$$ and N x N matrix A. Where



and


$$ A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1N} \\
a_{21} & a_{22} & \cdots & a_{2N} \\
\vdots & \vdots & \ddots & \vdots \\
a_{N1} & a_{N2} & \cdots & a_{NN}
\end{bmatrix} 
$$



```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
