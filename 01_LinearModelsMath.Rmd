---
title: "OLS Mathematical"
author: "H Emre Kale"
date: "1/17/2021"
output: html_document
header-includes:
   - \usepackage{amsmath}
   - \DeclareMathOperator{\dis}{d}
   - \newcommand{\betahat}{\hat\beta}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The linear regression model

Now we will explore the mathematical derivation of ordinary least squares (OLS) parameter estimation of the linear models in the form of:

$${\bf y} = X\beta + {\bf \epsilon}$$
where $y$ is the observation vector, $X$ is the design matrix, $\beta$'s are the coefficients and \epsilon is the error or in expanded form:

$$y_i = \sum_i\beta_jX_{ij} + \epsilon_i$$
According **Gauss-Markov Theorem** **O**rdinary **L**east **S**quares (OLS) estimation is the **B**est **L**inear **U**nbiased **E**stimator (BLUE) for the estimation of the parameters. The solution can be found by minimizing the sum of squared residual errors (RSS):


$$

\arg\min_{\beta} \epsilon^T\epsilon \text{, where} \\
\epsilon = y - X\beta \text{ so, } \\
RSS = \epsilon^T\epsilon = (y - X\beta)^T(y - X\beta)\\
$$


so our quadratic minimisation problem converts to:

$$\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}},(y - X\beta)^T(y - X\beta)$$
Let's write total squared error in matrix form and write all terms using distribution property. 

$$(y - X\beta)^T(y - X\beta) = y^Ty - y^TX\beta - (X\beta)^Ty+ (X\beta)^T X\beta$$

Remember $(AB)^T = B^TA^T$ so:

$$(y^T(X\beta))^T =  (X\beta)^T y$$
so the second and third term of the equation are transpose of each other and they are scalars. Since transpose of a scalar is the same scalar then, both of the terms are same so:

$$(y - X\beta)^T(y - X\beta) = y^Ty - 2(X\beta)^Ty + \beta^T X^T X\beta$$
This is a quadratic equation and in order to find the $\beta$ which minimizes this equation wrt $\beta$, we will take the partial derivative of the equation. We can find partial derivatives term by term.

We can easily see that the partial derivative is 0 for the first term:
$$\frac{\partial y^Ty}{\partial\beta} = 0$$
The open form of the second term is:

$$(X\beta)^T y = 
(\beta_1x_{11}y_1 +  \beta_2x_{12}y_1 + \cdots  \beta_px_{1p}y_1) +
(\beta_1x_{21}y_2 + \beta_2x_{22}y_2 + \cdots + \beta_px_{2p}y_2) + \cdots +
(\beta_1x_{N1}y_N + \beta_1x_{N2}y_N + \cdots + \beta_px_{Np}y_N)
$$

Then for eg. partial derivative of this equation wrt $\beta_1$ is:

$$\frac{\partial(X\beta)^T y}{\partial\beta_1} = x_{11}y_1 + x_{21}y_2 + \cdots + x_{N1}y_N$$ 

and in general form: 

$$\frac{\partial(X\beta)^T y}{\partial\beta_i} = x_{1i}y_1 + x_{2i}y_2 + \cdots + x_{Ni}y_N$$ 

then finally if put these terms in matrix form we see:

$$ 
\frac{\partial(X\beta)^T y}{\partial\beta} = \begin{bmatrix}
\frac{\partial(X\beta)^T y}{\partial\beta_1}\\
\frac{\partial(X\beta)^T y}{\partial\beta_2}\\
\vdots\\
\frac{\partial(X\beta)^T y}{\partial\beta_p} 
\end{bmatrix} 
= 
\begin{bmatrix}
x_{11}y_1 + x_{21}y_2 + \cdots + x_{N1}y_N \\x_{12}y_1 + x_{22}y_2 + \cdots + x_{N2}y_N \\  \vdots \\
x_{N2}y_1 + x_{N2}y_2 + \cdots + x_{Np}y_N 
\end{bmatrix}  = X^Ty
$$

The third equation is in the form $$\frac{\partial b^TAb}{\partial b}$$

where b is an m X 1 vector:

$$b = \begin{bmatrix}
b_1 & b_2 &\cdots& b_m \end{bmatrix}^T$$

and is a symmetric matrix:

$$ A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mm}
\end{bmatrix} 
$$

where $a_{ij} = a_{ji}$. Then:

$$b^TAb = 
b_1(b_1a_{11} + b_2a_{12} + \cdots + b_Na_{1m}) + b_2(b_1a_{21} + b_2a_{22} + \cdots + b_ma_{2m}) + \cdots  \\
+b_m(b_1a_{m1} + b_2a_{m2} + \cdots + b_Na_{mm})
$$
As we did before partial derivative of this equation wrt $\beta_1$ is: 

$$\frac{\partial\beta^TA\beta}{\partial b_1} = 2b_1a_{11} + (b_2a_{12}+ b_3a_{13} + \cdots + b_ma_{1m}) + (b_2a_{21} + b_3a_{31} + \cdots + b_ma_{m1} )$$ 
Since $a_{ij} = a_{ji}$ values are symmetric :

$$\frac{\partial\beta^TA\beta}{\partial b_1} = 2b_1a_{11} + 2b_2a_{12}  +  \cdots + 2b_ma_{1m}$$

generally:
$$\frac{\partial\beta^TA\beta}{\partial b_i} = 2b_1a_{i1} + 2b_2a_{i2}  +  \cdots + 2b_ma_{im}$$
in matrix form: 

$$ 
\frac{\partial b^TAb}{\partial b} = \begin{bmatrix}
\frac{\partial b^TAb}{\partial b_1}\\
\frac{\partial b^TAb}{\partial b_2}\\
\vdots\\
\frac{\partial b^TAb)^T y}{\partial b_m} 
\end{bmatrix} = 
\begin{bmatrix}
2b_1a_{11} + 2b_2a_{12}  +  \cdots + 2b_ma_{1m}\\ 
2b_1a_{21} + 2b_2a_{22}  +  \cdots + 2b_ma_{2m}\\
\vdots \\
2b_1a_{m1} + 2b_2a_{m2}  +  \cdots + 2b_ma_{mm}\\
\end{bmatrix}  = 2Ab
$$
If $A = X^TX$ and $b = \beta$ into this equation we see $\frac{\partial \beta X^TX\beta}{\partial \beta} = 2 X^TX\beta$. Then equation becomes

$$X^Ty - X^TX\beta = 0  \text{ and}\\
X^Ty = X^TX\beta$$

which is called the **normal equation** and then $\hat \beta$ is:

$$\hat\beta = (X^TX)^{-1}X^Ty $$

The matrix $(X^TX)^{-1}X^T$ is called the **Moore-Penrose psuedoinverse**. Then the estimated values are given:
$$
\hat y = X \hat \beta = X(X^TX)^{-1}X^Ty 
$$
This is a transformation from observed data ($\y$) to estimated data ($\hat y$) and the transformation matrix is

$$P = X(X^TX)^{-1}X^T$$
This projection matrix is also called *hat* matrix since it adds a hat on $y$.
# Prof of the solution minimizes the quadratic equation.

If you want to know is this solution really minimizes the error sum of squares go to  the proof. Best Linear Unbiased Estimator see prof [Gauss-Markov Theorem]. 


## Gauss-Markov Theorem

Now we will explore the mathematical derivation of ordinary least squares (OLS) parameter estimation of the linear models in the form of:

$${\bf y} = X\beta + {\bf \epsilon}$$
or in expanded form:

$$y_i = \sum_i\beta_jX_{ij} + \epsilon_i$$
According Gauss-Markov Theorem OLS is the {\bfB}est {\bfL}inear {\bfU}nbiased {\bfE}stimator. There are three assumptions of the Gauss-Markov theorem

1 The expectation value of the error $\epsilon$ is zero:
$$
E[\epsilon_i] = 0
$$
2 Errors ($\epsilon_i$) have identical and finite variance:
$$
Var(\epsilon_i) = \sigma < \infty
$$
3 Error components are uncorrelated
$$
Cov(\epsilon_i, \epsilon_j) = 0, \forall i \neq j
$$

## Linear Estimators

A linear estimator is defined as:
$$
\hat{\beta_i} = c_{1j}y_1 + c_{2j}y_2 + \cdots + c_{nj}y_n
$$
Note that $c$ could be only depend on values $X_{ij}$. if the expectation of the $\beta$ estimates are equal to the $\beta$:

$$E[\hat\beta] = \beta$$

## The Variance-Covariance Matrix of Estimates ($\hat \beta$)

REF : https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf

The variance-covariance matrix is defined as:

\begin{equation} 
Var(\hat \beta) = E((\betahat - E(\betahat))^T(\betahat - E(\betahat)))
\end{equation}

According to Gauss-Markov Theorem $\beta = E(\betahat)$. and remember $\hat\beta = (X^TX)^{-1}X^Ty$ and $y = X\beta + \epsilon$ so we can rewrite the previous equation:
\begin{equation}
\hat\beta = (X^TX)^{-1}X^T(X\beta + \epsilon) 
\end{equation}

Note that $(X^TX)^{-1}X^TX = I$ so we have:

$$
\betahat = I\beta + (X^TX)^{-1}X^T \epsilon\\
\betahat - \beta = (X^TX)^{-1}X^T \epsilon
$$
\begin{equation} 
E((\betahat - \beta))(\betahat - \beta)^T) =  E(((X^TX^{-1}X^T \epsilon)((X^TX^{-1}X^T \epsilon)^T) )\\
\end{equation}

We will take use of the property $(AB)^T =  B^TA^T$,

\begin{equation} 
E((\betahat - \beta))(\betahat - \beta)^T) =  E(((X^TX)^{-1}X^T) (\epsilon\epsilon^T)(X^T(X^TX)^{-T1})) )\\
\end{equation}




References:

Dobson, A.J. (2010) An Introduction to Generalized Linear Models, Second Edition, Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.

https://www.wikiwand.com/en/Gauss%E2%80%93Markov_theorem


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
