```{bash}
RETICULATE_MINICONDA_ENABLED=FALSE
```
```{python}
import os

```

<!-- http://r-statistics.co/Linear-Regression.html -->
  
Assumptions of linear regression:
    1- **Linearity**: The relationship between X and the mean of Y is linear.
    2 **Homoscedasticity**: The variance of residual is the same for any value of X.
    3- **Independence**: Observations are independent of each other.
Let's start with the built-in data set cars. cars data has 50 rows and two columns.

```{r}
setwd('/home/emre/fMRI/Codes_GH/DataScienceExamples')
head(cars)
str(cars)
```
## Exploratory Data Analysis

### Scatter Plot


```{r}
scatter.smooth(x=cars$speed, y=cars$dist, main="Dist ~ Speed",ylab = "Distance", xlab = "Speed")  # scatterplot
```
Plot shows there is a linear (the first assumption) and increasing relationship between two variables. 

### Box plot
```{r}
par(mfrow=c(1, 2))  # divide graph area in 2 columns
boxplot(cars$speed, main="Speed", sub=paste("Outlier value: ", boxplot.stats(cars$speed)$out))  # box plot for 'speed'
boxplot(cars$dist, main="Distance", sub=paste("Outlier value: ", boxplot.stats(cars$dist)$out))  # box plot for 'distance'

```

Distance has one outlier. Outliers can be detected usinq quantiles. 
IQR = 3rd Quantile - 1st Quantile
Outliers
For a value of independent variable (x) 
if x > 3rd Quantile + 1.5*IQR or if x < 1st Quantile - 1.5*IQR then x is outlier.

Now we check if there is outliers in data. We will use builtin **IQR** and **quantile** functions to create our own FindOutliersIQR() function.
**quantile** fucntion returns a named vector, and in our example the  for the speed variable **quantile** function print and return:

```{r}
q <-   quantile(cars$speed)
print(q)
```

For eg 1st Quantile can be obtained :

```{r}
q["25%"]
```

and IQR return the 3rdQuantile - 1stQuantile value:

```{r}
IQR(cars$speed)
```

The FindOutliersIQR function will use these functions and return outliers if there is any.

```{r}
FindOutliersIQR <- function(x) 
{
  x.quantile <- quantile(x) # cal
  x.IQR <- IQR(x)
  upper <- x.quantile["75%"]  + 1.5 * x.IQR
  lower <- x.quantile["25%"]  - 1.5 * x.IQR
  
  return(x[x < lower | x > upper])
}

speed.outliers <- FindOutliersIQR(cars$speed)
distance.outliers <- FindOutliersIQR(cars$dist)

print(paste0("Distance Outliers: ",distance.outliers))
print(paste0("Speed Outliers: ",speed.outliers))
```

It seems speed have one outlier value of 120

```{r}
par(mfrow=c(1, 2))  # divide graph area in 2 columns
hist(cars$speed, # histogram
 border="black",
 prob = TRUE, # show densities instead of frequencies
 xlab = "Speed",
 main = "Density and Histogram")
lines(density(cars$speed), # density plot
 lwd = 2, # thickness of line
 col = "blue")

hist(cars$dist, # histogram
 border="black",
 prob = TRUE, # show densities instead of frequencies
 xlab = "Distance",
 main = "Density and Histogram")
lines(density(cars$dist), # density plot
 lwd = 2, # thickness of line
 col = "blue")
#plot (density (cars$speed, na.rm=TRUE) )
```


## Linear Model
In this section, we will see how to built basic linear models using **lm()** and report results using **summary()** functions parameters and statistics of the model. We will start with the simplest model. The dependent varaible is dependent only on intercept.

### Only Intercept Model

First start with a simple model. Where Distance is modeled as a constant value $\beta_0$ that we want to estimate:

$$y = \beta_0 + \epsilon$$

As the first step we create a function that creates a line equation from estimated coefficients and intercept. 

```{r}
lm_eq <- function(model, yname = 'y' ) {
  
  a = format(unname(coef(model)[1]), digits = 3)
  b = format(unname(coef(model)[2]), digits = 3)
  r2 = format(summary(model)$r.squared, digits = 3)
  cc <-  coef(model)
  eqn <- paste(yname," = ", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e")
  return(eqn)
  #return(paste0(yname, ' = ',a, ' + ', b,'*', xname,'\n','R^2 = ',r2))
}
```

Next we create our model with only intercept plot the line on the scatter plot.

```{r}

cars.simple <- lm(cars$dist ~ 1)

with(cars,(plot(x = speed, y = dist,xlab = "Speed", ylab = "Distance")))
lines(cars$speed,fitted(cars.simple))
text(x=8, y = 110, labels=lm_eq(cars.simple))
```

```{r}
summary.cars.simple <- summary(cars.simple)
```

## Comparison of Only intercept term to one sample t-test. 

Using only intercept is the equivalent of hypotheses testing:

$H_0$ : true mean of speed is equal to zero

$H_1$ : true mean of speed is not equal to zero
  
This could be done with one-sample t-test:

```{r}
t.test(x = cars$dist)
print(summary(cars.simple) )
```

The p values (6.38e-16) are equal and estimated mean (42.980 ) is also same as it should be.

## Linear Model with only one dependent variable

The linear model for one exploratory variable is: 

$$y_i = \beta_{0i} + \beta_{1i}x$$
In our example:

$$Distance = \beta_0 + \beta_1 * Speed$$

To see, if we want to check if there is a linear relation between two variables, then we should check the slope of the model $\beta_1$ should be different then zero. So our hypothesis is:

$H_0 :\beta_1 =  0$

$H_1 :\beta_1 \neq 0$

First step is to fit the model and check the results by printing the model:

```{r}
cars.lm <- lm(dist ~ speed, data=cars)
print(cars.lm)
```

```{r}
cars.lm <- lm(dist ~ speed, data=cars)
print(cars.lm)
```

When we check the coefficients of the model we see that our estimated fit is:
$$Distance =  -17.579    +  3.932 * Speed$$

Similarly as we did on previous section, we can visualise the fit on the scatter plot.

```{r}
with(cars,(plot(x = speed, y = dist,xlab = "Speed", ylab = "Distance")))
lines(cars$speed,fitted(cars.lm))
text(x=8, y = 110, labels=lm_eq(cars.lm))
```

which means one unit change in speed increase distance 3.932 units. Using summary function we can look at statistics.

```{r}


cars.lm.summary <- summary(cars.lm)

coef(cars.lm.summary)

# faraway.summary <- faraway::sumary
# 
# faraway.summary(cars.lm)
# faraway.summary(cars.simple)

#xtxi <- cars.lm$cov.unscaled
#sqrt (diag (xtxi)) *cars.lm.sd

cars.lm.coefs <-cars.lm.summary$coefficients 
cars.lm.sd <- sqrt(deviance(cars.lm) / df.residual(cars.lm))

cars.lm.beta.estimate <- cars.lm.coefs["speed", "Estimate"]  # get beta estimate for speed
cars.lm.stderror <- cars.lm.coefs["speed", "Std. Error"]  # get std.error for speed

t_value <- cars.lm.beta.estimate/cars.lm.stderror  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(cars)-ncol(cars))  

print(t_value)
print(p_value)


f <- cars.lm.summary$fstatistic  # parameters for model p-value calc
model_p <- pf(f[1], f[2], f[3], lower=FALSE)
print(model_p)
```

\newcommand*\mean[1]{\bar{#1}}

# Model Evaluation

## $R^2$ and $R^2_{adjusted}$
When the model assumptions for linear model is met then either $R^2$ or $R^2_{adjusted}$ can be used to measure goodness-of-fit
Remember sum of squares (SSE) is: 
$$SSE = \sum_i{(y_i - \hat{y_i})^2}{}$$
where $y_i$ is each data and $\hat{y}_i$ is the model value. And total sum of squares (SST)  is :
$$SST = \sum_i{(y_i - {\mean{y_i}})^2}{}$$
SST is basically variance of the data and SSE is the variance of the residuals (unexplained). Then $R^2$ is the explained variance to total variance of the data. The closer the $R^2$ to 1 is better.
$$R^2 = 1 - \frac{SSE}{SST}$$
$R^2$ value can be obtained from the summary of the model : 
```{r}
cars.lm.summary$r.squared
```

$R^2$ 


## Model fitting using linear algebra (optional)

Reference 1: http://r-statistics.co/Linear-Regression.html
  basic examples
Reference 2: From Faraday linear models with R
  linear algebra calculations
Reference 3: https://www.r-bloggers.com/2012/09/histogram-density-plot-combo-in-r/
  histograms
Reference 4: https://www.r-graph-gallery.com/44-polynomial-curve-fitting.html

Calculate $$X^tX$$ and then $$\beta = {(X^tX)}^{-1}X^ty$$:
```{r}
X <- model.matrix ( ~ speed, data = cars)
y <- cars$dist
xtxi <- solve(t (X) %*% X)

beta <- solve (crossprod (X, X), crossprod (X, y))
cars.residuals <- y - X %*% beta 

hist(cars.residuals, # histogram
 border="black",
 prob = TRUE, # show densities instead of frequencies
 xlab = "Speed",
 main = "Density and Histogram")
lines(density(cars.residuals), # density plot
 lwd = 2, # thickness of line
 col = "blue")
```
```{r}
sd <- sqrt(crossprod(cars.lm.summary$residuals,cars.lm.summary$residuals)/48)
```

## Solve using QR decomposition

QR decomposition is a more stable method to solve equation.

Buraya farawayden ve HARPtan cozumu koyalim.


## Sandwich Models and OLS

 http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r/




